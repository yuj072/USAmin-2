{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = './crawl_embedding/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = './glove_embedding/glove.840B.300d.txt'\n",
    "EMBED_SIZE = 600\n",
    "MAX_FEATURES = 100000\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "def preprocess(data):\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "x_train = preprocess(train['overview'])\n",
    "y_train = np.asarray(train['vote_average'])\n",
    "\n",
    "x_test = preprocess(test['overview'])\n",
    "x_actual = np.asarray(test['vote_average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "#Pad the sequences\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:29, 10462.07it/s]\n",
      "1999996it [03:06, 10739.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logic to build the embedding matrix taken and modified from:\n",
    "#    https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return embedding_matrix\n",
    "\n",
    "glove_matrix = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "crawl_matrix = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "\n",
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "\n",
    "del glove_matrix\n",
    "del crawl_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM and MLP Definition\n",
    "units = 64\n",
    "def custom_LSTM(embedding_matrix):\n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "    x = Embedding(embedding_matrix.shape[0], EMBED_SIZE, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.5)(x)\n",
    "    x, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(units, return_sequences=True, return_state=True))(x)\n",
    "    h_state = concatenate([forward_h, backward_h])\n",
    "    c_state = concatenate([forward_c, backward_c])\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = concatenate([avg_pool, max_pool, h_state, c_state])\n",
    "    \n",
    "    #MLP Definition\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    \n",
    "    #Compiling the models together\n",
    "    model=Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='mean_absolute_percentage_error', optimizer=Adam(lr=1e-3, decay=0), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\David\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\David\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\David\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\David\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 3032 samples, validate on 759 samples\n",
      "Epoch 1/3\n",
      "3032/3032 [==============================] - ETA: 33s - loss: 100.0000 - acc: 0.0000e+ - ETA: 21s - loss: 99.9995 - acc: 0.0000e+00 - ETA: 14s - loss: 99.9445 - acc: 0.0000e+0 - ETA: 8s - loss: 98.8496 - acc: 0.0000e+0 - ETA: 4s - loss: 97.0609 - acc: 0.0000e+ - 29s 10ms/step - loss: 94.5797 - acc: 0.0000e+00 - val_loss: 70.4136 - val_acc: 0.0026\n",
      "Epoch 2/3\n",
      "3032/3032 [==============================] - ETA: 18s - loss: 70.1190 - acc: 0.0000e+0 - ETA: 15s - loss: 61.6871 - acc: 9.7656e-0 - ETA: 11s - loss: 52.7214 - acc: 0.0046    - ETA: 7s - loss: 45.6503 - acc: 0.005 - ETA: 3s - loss: 44.2918 - acc: 0.00 - 26s 8ms/step - loss: 43.6477 - acc: 0.0073 - val_loss: 28.1686 - val_acc: 0.0303\n",
      "Epoch 3/3\n",
      "3032/3032 [==============================] - ETA: 19s - loss: 27.2392 - acc: 0.031 - ETA: 15s - loss: 24.1258 - acc: 0.034 - ETA: 11s - loss: 23.0354 - acc: 0.031 - ETA: 7s - loss: 23.1464 - acc: 0.029 - ETA: 3s - loss: 23.6723 - acc: 0.02 - 25s 8ms/step - loss: 23.9285 - acc: 0.0257 - val_loss: 23.2952 - val_acc: 0.0198\n",
      "949/949 [==============================] - ETA:  - 4s 4ms/step\n",
      "Train on 3033 samples, validate on 758 samples\n",
      "Epoch 1/3\n",
      "3033/3033 [==============================] - ETA: 19s - loss: 22.0723 - acc: 0.013 - ETA: 15s - loss: 19.9076 - acc: 0.028 - ETA: 11s - loss: 18.8545 - acc: 0.032 - ETA: 7s - loss: 19.2895 - acc: 0.032 - ETA: 3s - loss: 19.2416 - acc: 0.03 - 26s 8ms/step - loss: 19.2092 - acc: 0.0326 - val_loss: 14.9589 - val_acc: 0.0303\n",
      "Epoch 2/3\n",
      "3033/3033 [==============================] - ETA: 18s - loss: 18.9108 - acc: 0.046 - ETA: 15s - loss: 17.0149 - acc: 0.050 - ETA: 11s - loss: 16.7478 - acc: 0.043 - ETA: 7s - loss: 16.9249 - acc: 0.040 - ETA: 3s - loss: 16.9599 - acc: 0.03 - 26s 8ms/step - loss: 16.8847 - acc: 0.0379 - val_loss: 16.5471 - val_acc: 0.0237\n",
      "Epoch 3/3\n",
      "3033/3033 [==============================] - ETA: 18s - loss: 14.4294 - acc: 0.054 - ETA: 15s - loss: 14.5500 - acc: 0.053 - ETA: 11s - loss: 14.8119 - acc: 0.051 - ETA: 7s - loss: 15.6967 - acc: 0.048 - ETA: 3s - loss: 15.3922 - acc: 0.04 - 26s 9ms/step - loss: 15.9193 - acc: 0.0452 - val_loss: 16.3551 - val_acc: 0.0264\n",
      "949/949 [==============================] - ETA:  - 4s 4ms/step\n",
      "Train on 3033 samples, validate on 758 samples\n",
      "Epoch 1/3\n",
      "3033/3033 [==============================] - ETA: 19s - loss: 14.4615 - acc: 0.023 - ETA: 15s - loss: 14.6250 - acc: 0.029 - ETA: 11s - loss: 14.9052 - acc: 0.028 - ETA: 7s - loss: 15.4889 - acc: 0.031 - ETA: 3s - loss: 15.2802 - acc: 0.03 - 26s 8ms/step - loss: 15.2980 - acc: 0.0346 - val_loss: 14.7999 - val_acc: 0.0501\n",
      "Epoch 2/3\n",
      "3033/3033 [==============================] - ETA: 18s - loss: 14.9274 - acc: 0.054 - ETA: 15s - loss: 15.1710 - acc: 0.052 - ETA: 11s - loss: 15.0127 - acc: 0.046 - ETA: 7s - loss: 15.1542 - acc: 0.044 - ETA: 3s - loss: 14.8260 - acc: 0.04 - 26s 9ms/step - loss: 14.8230 - acc: 0.0412 - val_loss: 17.2023 - val_acc: 0.0251\n",
      "Epoch 3/3\n",
      "3033/3033 [==============================] - ETA: 19s - loss: 14.0545 - acc: 0.037 - ETA: 15s - loss: 13.6609 - acc: 0.046 - ETA: 11s - loss: 14.0075 - acc: 0.042 - ETA: 7s - loss: 14.0989 - acc: 0.038 - ETA: 3s - loss: 14.5043 - acc: 0.03 - 26s 8ms/step - loss: 14.4077 - acc: 0.0376 - val_loss: 15.0104 - val_acc: 0.0488\n",
      "949/949 [==============================] - ETA:  - 4s 4ms/step\n",
      "Train on 3033 samples, validate on 758 samples\n",
      "Epoch 1/3\n",
      "3033/3033 [==============================] - ETA: 19s - loss: 13.7144 - acc: 0.043 - ETA: 15s - loss: 13.7517 - acc: 0.044 - ETA: 11s - loss: 14.0439 - acc: 0.044 - ETA: 7s - loss: 14.0921 - acc: 0.044 - ETA: 3s - loss: 14.1399 - acc: 0.04 - 26s 8ms/step - loss: 14.3298 - acc: 0.0429 - val_loss: 14.7544 - val_acc: 0.0343\n",
      "Epoch 2/3\n",
      "3033/3033 [==============================] - ETA: 18s - loss: 15.8649 - acc: 0.039 - ETA: 15s - loss: 15.3189 - acc: 0.044 - ETA: 11s - loss: 14.8027 - acc: 0.042 - ETA: 7s - loss: 14.7543 - acc: 0.043 - ETA: 3s - loss: 14.4720 - acc: 0.04 - 26s 9ms/step - loss: 14.2075 - acc: 0.0455 - val_loss: 15.9698 - val_acc: 0.0303\n",
      "Epoch 3/3\n",
      "3033/3033 [==============================] - ETA: 19s - loss: 13.6817 - acc: 0.052 - ETA: 15s - loss: 14.3260 - acc: 0.058 - ETA: 11s - loss: 14.5873 - acc: 0.052 - ETA: 7s - loss: 13.8382 - acc: 0.053 - ETA: 3s - loss: 13.7958 - acc: 0.05 - 26s 9ms/step - loss: 13.9410 - acc: 0.0488 - val_loss: 14.6488 - val_acc: 0.0356\n",
      "949/949 [==============================] - ETA:  - 4s 4ms/step\n",
      "Train on 3033 samples, validate on 758 samples\n",
      "Epoch 1/3\n",
      "3033/3033 [==============================] - ETA: 19s - loss: 14.1167 - acc: 0.044 - ETA: 15s - loss: 14.2236 - acc: 0.041 - ETA: 12s - loss: 14.1236 - acc: 0.038 - ETA: 7s - loss: 13.8603 - acc: 0.041 - ETA: 3s - loss: 13.7013 - acc: 0.04 - 26s 9ms/step - loss: 13.6888 - acc: 0.0458 - val_loss: 13.7992 - val_acc: 0.0409\n",
      "Epoch 2/3\n",
      "3033/3033 [==============================] - ETA: 18s - loss: 13.6701 - acc: 0.058 - ETA: 14s - loss: 13.4973 - acc: 0.055 - ETA: 11s - loss: 13.2819 - acc: 0.050 - ETA: 7s - loss: 13.3886 - acc: 0.052 - ETA: 3s - loss: 13.5255 - acc: 0.05 - 26s 9ms/step - loss: 13.5823 - acc: 0.0501 - val_loss: 14.9867 - val_acc: 0.0396\n",
      "Epoch 3/3\n",
      "3033/3033 [==============================] - ETA: 18s - loss: 15.2065 - acc: 0.048 - ETA: 15s - loss: 13.6658 - acc: 0.052 - ETA: 11s - loss: 13.4485 - acc: 0.050 - ETA: 7s - loss: 13.4690 - acc: 0.046 - ETA: 3s - loss: 13.5365 - acc: 0.04 - 26s 9ms/step - loss: 13.2814 - acc: 0.0488 - val_loss: 16.1783 - val_acc: 0.0343\n",
      "949/949 [==============================] - ETA:  - 3s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "#Validation using KFold\n",
    "num_folds = 5\n",
    "num_epochs = 3\n",
    "folds = KFold(n_splits=num_folds, shuffle=True)\n",
    "model = custom_LSTM(embedding_matrix)\n",
    "def train(x_train, y_train, x_test):\n",
    "    prediction = np.zeros((len(x_test), 1))\n",
    "    \n",
    "    #KFold Validation\n",
    "    for fold_index, (train_index, valid_index) in enumerate(folds.split(x_train, y_train)):\n",
    "        x_train_split = x_train[train_index]\n",
    "        y_train_split = y_train[train_index]\n",
    "        x_validation = x_train[valid_index]\n",
    "        y_validation = y_train[valid_index]\n",
    "        \n",
    "        model.fit(x_train_split, y_train_split, batch_size=512, epochs = num_epochs, validation_data = (x_validation, y_validation))\n",
    "        \n",
    "        prediction += model.predict(x_test, batch_size = 512, verbose = 1)\n",
    "        \n",
    "    prediction /= fold_index\n",
    "    \n",
    "    return prediction\n",
    "prediction = train(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.03029537]\n",
      " [6.63233602]\n",
      " [7.05228829]\n",
      " [6.73217261]\n",
      " [7.13432753]\n",
      " [6.43455625]\n",
      " [6.77892661]\n",
      " [7.06108618]\n",
      " [7.2117548 ]\n",
      " [6.54170299]\n",
      " [6.87599349]\n",
      " [6.79725146]\n",
      " [6.2257911 ]\n",
      " [6.68903315]\n",
      " [6.17887414]\n",
      " [6.40115786]\n",
      " [6.74483383]\n",
      " [7.1735307 ]\n",
      " [6.5861243 ]\n",
      " [7.01486254]\n",
      " [7.06796134]\n",
      " [6.9933579 ]\n",
      " [6.66255701]\n",
      " [6.82735825]\n",
      " [6.23902273]\n",
      " [6.41591859]\n",
      " [7.17097592]\n",
      " [6.67372251]\n",
      " [6.87125349]\n",
      " [6.18485248]\n",
      " [6.65235078]\n",
      " [6.33224952]\n",
      " [7.10833359]\n",
      " [6.63595676]\n",
      " [7.08970797]\n",
      " [5.88676286]\n",
      " [6.88455391]\n",
      " [6.80971384]\n",
      " [5.86179185]\n",
      " [6.78562677]\n",
      " [6.27749026]\n",
      " [6.45777321]\n",
      " [6.95089734]\n",
      " [6.88942325]\n",
      " [6.43518114]\n",
      " [6.84863853]\n",
      " [6.39019275]\n",
      " [6.39884913]\n",
      " [6.90913022]\n",
      " [6.92496407]\n",
      " [6.8880682 ]\n",
      " [6.31056643]\n",
      " [6.95201278]\n",
      " [6.79368496]\n",
      " [6.71131229]\n",
      " [6.65659988]\n",
      " [6.77406585]\n",
      " [6.74157882]\n",
      " [7.11001289]\n",
      " [6.74103391]\n",
      " [6.4058429 ]\n",
      " [7.15133369]\n",
      " [7.31707013]\n",
      " [6.58558393]\n",
      " [6.39478326]\n",
      " [7.13413215]\n",
      " [6.32456958]\n",
      " [6.75087464]\n",
      " [7.08661711]\n",
      " [7.07520556]\n",
      " [6.38419724]\n",
      " [7.14024818]\n",
      " [6.86650693]\n",
      " [6.86056721]\n",
      " [7.03302586]\n",
      " [7.00362563]\n",
      " [6.81495166]\n",
      " [6.63503325]\n",
      " [6.47542703]\n",
      " [6.72024405]\n",
      " [6.33044052]\n",
      " [6.67063439]\n",
      " [6.89164257]\n",
      " [7.08045769]\n",
      " [6.84208858]\n",
      " [6.36731827]\n",
      " [5.8188374 ]\n",
      " [6.74262071]\n",
      " [6.76606071]\n",
      " [6.31418133]\n",
      " [6.83055723]\n",
      " [6.81783855]\n",
      " [6.72164416]\n",
      " [6.82380843]\n",
      " [6.61312699]\n",
      " [6.49227631]\n",
      " [7.09631741]\n",
      " [6.38593006]\n",
      " [6.72275722]\n",
      " [6.21542555]\n",
      " [6.37928629]\n",
      " [7.16188443]\n",
      " [6.32029176]\n",
      " [6.14025855]\n",
      " [6.50005209]\n",
      " [7.07166052]\n",
      " [6.29541397]\n",
      " [6.47106588]\n",
      " [6.93325353]\n",
      " [7.08015311]\n",
      " [6.82531333]\n",
      " [6.69259834]\n",
      " [7.21054089]\n",
      " [6.86236644]\n",
      " [6.64206839]\n",
      " [7.22372663]\n",
      " [6.29204702]\n",
      " [7.17855072]\n",
      " [6.53575337]\n",
      " [6.53399932]\n",
      " [6.81629789]\n",
      " [6.83504748]\n",
      " [6.88324964]\n",
      " [6.66338348]\n",
      " [6.17209625]\n",
      " [6.71279931]\n",
      " [7.01225626]\n",
      " [6.39900255]\n",
      " [6.86670506]\n",
      " [6.40929222]\n",
      " [6.56309199]\n",
      " [6.41465485]\n",
      " [6.77424967]\n",
      " [6.49106419]\n",
      " [7.40207696]\n",
      " [6.84930348]\n",
      " [6.35090077]\n",
      " [6.81197858]\n",
      " [6.77227879]\n",
      " [6.9655602 ]\n",
      " [6.93178678]\n",
      " [6.79071438]\n",
      " [6.84829402]\n",
      " [6.98882413]\n",
      " [6.95240211]\n",
      " [6.07624447]\n",
      " [6.46566916]\n",
      " [6.45977569]\n",
      " [6.54781628]\n",
      " [6.74553883]\n",
      " [6.73972201]\n",
      " [6.63959742]\n",
      " [6.69197285]\n",
      " [6.81982696]\n",
      " [6.91343534]\n",
      " [7.4026407 ]\n",
      " [6.74152851]\n",
      " [6.82685781]\n",
      " [6.7073257 ]\n",
      " [6.5393548 ]\n",
      " [6.67927539]\n",
      " [6.59866118]\n",
      " [6.36350834]\n",
      " [6.39165938]\n",
      " [7.0878824 ]\n",
      " [6.64457822]\n",
      " [6.59317696]\n",
      " [6.88362491]\n",
      " [6.78416431]\n",
      " [6.76814389]\n",
      " [6.66467559]\n",
      " [6.89865804]\n",
      " [6.51467073]\n",
      " [6.54169524]\n",
      " [6.55948973]\n",
      " [6.70039093]\n",
      " [6.58511281]\n",
      " [7.06292868]\n",
      " [6.54611909]\n",
      " [6.56833708]\n",
      " [6.99405038]\n",
      " [6.47441876]\n",
      " [6.58230305]\n",
      " [6.87897468]\n",
      " [6.5643183 ]\n",
      " [6.81840348]\n",
      " [6.2939806 ]\n",
      " [6.62098122]\n",
      " [6.76200199]\n",
      " [6.53818715]\n",
      " [6.92708325]\n",
      " [7.02262175]\n",
      " [6.45129693]\n",
      " [6.63056469]\n",
      " [6.85295415]\n",
      " [6.24890196]\n",
      " [6.74144447]\n",
      " [7.02658725]\n",
      " [7.00585973]\n",
      " [6.79085946]\n",
      " [6.5963099 ]\n",
      " [6.34158278]\n",
      " [6.43746543]\n",
      " [6.66622865]\n",
      " [6.75558007]\n",
      " [6.93311012]\n",
      " [6.78054488]\n",
      " [7.04225576]\n",
      " [6.514624  ]\n",
      " [6.5782212 ]\n",
      " [6.74683273]\n",
      " [6.76465905]\n",
      " [6.66145694]\n",
      " [6.71136105]\n",
      " [6.73405743]\n",
      " [6.59583652]\n",
      " [6.69000959]\n",
      " [6.6077863 ]\n",
      " [6.1158545 ]\n",
      " [6.77655733]\n",
      " [6.67167699]\n",
      " [6.96679366]\n",
      " [6.55643165]\n",
      " [6.90560889]\n",
      " [6.20643139]\n",
      " [6.70627606]\n",
      " [6.28288233]\n",
      " [6.87614906]\n",
      " [6.50429487]\n",
      " [6.6476053 ]\n",
      " [6.24435115]\n",
      " [6.8862232 ]\n",
      " [6.49965417]\n",
      " [6.495893  ]\n",
      " [6.37083137]\n",
      " [6.18208802]\n",
      " [6.2465421 ]\n",
      " [6.26573992]\n",
      " [7.02256644]\n",
      " [6.65956759]\n",
      " [6.39099824]\n",
      " [6.43931532]\n",
      " [6.77437985]\n",
      " [6.56968617]\n",
      " [7.02132738]\n",
      " [6.76219368]\n",
      " [7.11928272]\n",
      " [6.44449806]\n",
      " [6.80379021]\n",
      " [6.57142615]\n",
      " [6.57319796]\n",
      " [7.15317392]\n",
      " [6.64190865]\n",
      " [6.77432895]\n",
      " [6.4639194 ]\n",
      " [7.06750202]\n",
      " [6.76507974]\n",
      " [6.44198453]\n",
      " [6.79305673]\n",
      " [7.12491393]\n",
      " [6.83452141]\n",
      " [7.28538787]\n",
      " [7.36231077]\n",
      " [6.71910262]\n",
      " [6.62265861]\n",
      " [7.13812685]\n",
      " [6.4948709 ]\n",
      " [6.70746291]\n",
      " [6.50086367]\n",
      " [7.10978818]\n",
      " [6.81852293]\n",
      " [6.66795969]\n",
      " [6.47173452]\n",
      " [6.63644671]\n",
      " [6.98253465]\n",
      " [6.7726258 ]\n",
      " [6.5654887 ]\n",
      " [6.51510274]\n",
      " [6.84315252]\n",
      " [6.67441297]\n",
      " [6.08420813]\n",
      " [6.7057364 ]\n",
      " [6.91380143]\n",
      " [6.41787624]\n",
      " [6.54983497]\n",
      " [7.03061199]\n",
      " [7.01846981]\n",
      " [6.8279891 ]\n",
      " [6.58979678]\n",
      " [6.7128135 ]\n",
      " [6.39653885]\n",
      " [6.67997396]\n",
      " [6.04390192]\n",
      " [6.46102417]\n",
      " [6.98561871]\n",
      " [6.41022336]\n",
      " [6.65422976]\n",
      " [6.79106402]\n",
      " [7.09856772]\n",
      " [6.23412383]\n",
      " [6.13199723]\n",
      " [6.4911406 ]\n",
      " [7.25474179]\n",
      " [6.64517009]\n",
      " [6.59137392]\n",
      " [6.74380744]\n",
      " [6.73721218]\n",
      " [6.67707765]\n",
      " [6.41138649]\n",
      " [6.7818774 ]\n",
      " [6.90258467]\n",
      " [7.17180371]\n",
      " [7.07964635]\n",
      " [6.86288941]\n",
      " [6.85479343]\n",
      " [7.00472784]\n",
      " [6.53257322]\n",
      " [6.78559291]\n",
      " [6.55206859]\n",
      " [6.43727672]\n",
      " [6.4312948 ]\n",
      " [7.0473516 ]\n",
      " [6.46984506]\n",
      " [7.13605011]\n",
      " [7.01902723]\n",
      " [6.35355699]\n",
      " [6.94383097]\n",
      " [6.71860099]\n",
      " [7.03912866]\n",
      " [6.72875643]\n",
      " [7.08174658]\n",
      " [6.90399253]\n",
      " [6.684219  ]\n",
      " [6.66828728]\n",
      " [6.55581617]\n",
      " [6.63586974]\n",
      " [6.22196329]\n",
      " [6.8748194 ]\n",
      " [7.23614466]\n",
      " [6.87648869]\n",
      " [6.23604095]\n",
      " [6.69200122]\n",
      " [6.98396862]\n",
      " [6.48032272]\n",
      " [7.28993964]\n",
      " [6.96999121]\n",
      " [6.71427965]\n",
      " [7.30432892]\n",
      " [6.62765229]\n",
      " [6.57149947]\n",
      " [6.67245984]\n",
      " [7.00296032]\n",
      " [6.62844205]\n",
      " [6.93114221]\n",
      " [6.62967038]\n",
      " [6.94567096]\n",
      " [6.64324307]\n",
      " [7.09318733]\n",
      " [7.22897506]\n",
      " [6.59153891]\n",
      " [6.45374632]\n",
      " [6.15328848]\n",
      " [6.2871812 ]\n",
      " [6.43848312]\n",
      " [6.99023616]\n",
      " [6.63340569]\n",
      " [6.73538828]\n",
      " [6.88730145]\n",
      " [6.66443145]\n",
      " [6.68970096]\n",
      " [6.92606831]\n",
      " [7.08909237]\n",
      " [6.52169621]\n",
      " [6.62749124]\n",
      " [6.77171838]\n",
      " [6.57190335]\n",
      " [6.6141187 ]\n",
      " [6.92123759]\n",
      " [6.80775249]\n",
      " [6.63078046]\n",
      " [6.52915871]\n",
      " [6.63247216]\n",
      " [6.77100444]\n",
      " [6.66776562]\n",
      " [6.72622657]\n",
      " [6.15300965]\n",
      " [6.63541222]\n",
      " [6.74461663]\n",
      " [6.93383837]\n",
      " [7.21275747]\n",
      " [7.07570565]\n",
      " [6.58403647]\n",
      " [6.72646689]\n",
      " [6.9841553 ]\n",
      " [6.4719795 ]\n",
      " [6.58454561]\n",
      " [7.17799246]\n",
      " [6.38741112]\n",
      " [6.80023551]\n",
      " [7.14765286]\n",
      " [7.0755893 ]\n",
      " [6.62023211]\n",
      " [6.90432322]\n",
      " [6.37595594]\n",
      " [6.93420351]\n",
      " [6.5118618 ]\n",
      " [7.12098336]\n",
      " [7.03857899]\n",
      " [6.50780475]\n",
      " [7.00582087]\n",
      " [7.18365085]\n",
      " [6.78502285]\n",
      " [7.36273634]\n",
      " [5.66658199]\n",
      " [6.76743174]\n",
      " [6.94614422]\n",
      " [6.54286349]\n",
      " [7.12186527]\n",
      " [6.39399314]\n",
      " [7.17025781]\n",
      " [6.87968588]\n",
      " [6.55126047]\n",
      " [6.46135008]\n",
      " [6.43611515]\n",
      " [7.05649495]\n",
      " [6.47926652]\n",
      " [6.80622447]\n",
      " [6.27227426]\n",
      " [6.34198976]\n",
      " [7.51550698]\n",
      " [6.57515109]\n",
      " [6.48326242]\n",
      " [6.60346675]\n",
      " [6.23189723]\n",
      " [7.09801841]\n",
      " [6.13466334]\n",
      " [7.30800438]\n",
      " [6.3294692 ]\n",
      " [6.9034164 ]\n",
      " [6.58530104]\n",
      " [6.45489192]\n",
      " [7.07214725]\n",
      " [6.66010964]\n",
      " [6.94169903]\n",
      " [6.4188602 ]\n",
      " [6.83703744]\n",
      " [6.48162735]\n",
      " [6.6944989 ]\n",
      " [6.80623925]\n",
      " [6.69579601]\n",
      " [6.73102808]\n",
      " [7.02671659]\n",
      " [6.8827821 ]\n",
      " [6.74494755]\n",
      " [6.92987227]\n",
      " [7.09408998]\n",
      " [6.43122077]\n",
      " [6.67484307]\n",
      " [6.69202769]\n",
      " [6.63677037]\n",
      " [6.80179071]\n",
      " [6.95994353]\n",
      " [6.84381533]\n",
      " [6.72480512]\n",
      " [6.69118214]\n",
      " [6.92034733]\n",
      " [6.99963975]\n",
      " [6.466537  ]\n",
      " [6.75711405]\n",
      " [7.28421009]\n",
      " [6.76717317]\n",
      " [6.82587159]\n",
      " [6.73328173]\n",
      " [6.41049409]\n",
      " [6.68661273]\n",
      " [7.35607421]\n",
      " [6.68389964]\n",
      " [6.54503703]\n",
      " [7.15008652]\n",
      " [6.32244635]\n",
      " [6.91793406]\n",
      " [6.77699924]\n",
      " [6.85263121]\n",
      " [6.7833643 ]\n",
      " [7.08685887]\n",
      " [5.93923122]\n",
      " [7.00468338]\n",
      " [7.06081021]\n",
      " [6.56432283]\n",
      " [6.69529343]\n",
      " [6.59085381]\n",
      " [6.7790755 ]\n",
      " [6.30930769]\n",
      " [6.75194359]\n",
      " [6.70745707]\n",
      " [7.28976798]\n",
      " [6.95486212]\n",
      " [6.5037992 ]\n",
      " [6.41409981]\n",
      " [7.08393502]\n",
      " [7.37946236]\n",
      " [6.38312507]\n",
      " [6.72767282]\n",
      " [6.75364172]\n",
      " [6.80939865]\n",
      " [6.4138248 ]\n",
      " [7.14802194]\n",
      " [6.0696432 ]\n",
      " [6.68189526]\n",
      " [6.23659766]\n",
      " [7.04513407]\n",
      " [6.87736881]\n",
      " [6.82468927]\n",
      " [6.62110782]\n",
      " [6.61696208]\n",
      " [7.03762519]\n",
      " [6.63693571]\n",
      " [7.12142205]\n",
      " [6.67203045]\n",
      " [6.72175562]\n",
      " [6.63160288]\n",
      " [6.23176479]\n",
      " [6.44564521]\n",
      " [6.31471193]\n",
      " [7.23308849]\n",
      " [6.54965949]\n",
      " [6.5801599 ]\n",
      " [6.75282156]\n",
      " [6.92258716]\n",
      " [7.00292194]\n",
      " [6.73178971]\n",
      " [6.91356301]\n",
      " [6.89632261]\n",
      " [6.70757365]\n",
      " [6.79424667]\n",
      " [6.81549501]\n",
      " [6.8422097 ]\n",
      " [6.61965621]\n",
      " [6.64006054]\n",
      " [6.36405361]\n",
      " [7.09629381]\n",
      " [6.35507894]\n",
      " [6.82626534]\n",
      " [6.33623219]\n",
      " [7.14814174]\n",
      " [6.45483696]\n",
      " [5.95472229]\n",
      " [6.75782382]\n",
      " [7.00035334]\n",
      " [7.48276079]\n",
      " [6.77584434]\n",
      " [6.5520103 ]\n",
      " [7.5054636 ]\n",
      " [6.76179063]\n",
      " [6.52942741]\n",
      " [6.19625312]\n",
      " [7.10499847]\n",
      " [7.0300113 ]\n",
      " [6.70280123]\n",
      " [6.92222369]\n",
      " [6.75636101]\n",
      " [6.68058431]\n",
      " [7.25388241]\n",
      " [6.68366265]\n",
      " [6.68309116]\n",
      " [6.70729876]\n",
      " [6.59636271]\n",
      " [6.59399951]\n",
      " [6.96840155]\n",
      " [6.97392082]\n",
      " [6.58226216]\n",
      " [6.74804819]\n",
      " [6.93228292]\n",
      " [6.93919325]\n",
      " [6.84240401]\n",
      " [7.03586924]\n",
      " [7.12305009]\n",
      " [6.90952659]\n",
      " [6.81893837]\n",
      " [6.7291652 ]\n",
      " [6.91468322]\n",
      " [6.90126181]\n",
      " [6.66070282]\n",
      " [6.52235425]\n",
      " [6.90879357]\n",
      " [6.96396136]\n",
      " [6.73648727]\n",
      " [6.27129209]\n",
      " [6.6919961 ]\n",
      " [6.43293118]\n",
      " [6.77191973]\n",
      " [6.74216461]\n",
      " [6.56820941]\n",
      " [6.87523377]\n",
      " [6.26005304]\n",
      " [7.00450945]\n",
      " [6.58995247]\n",
      " [6.38756347]\n",
      " [6.7497201 ]\n",
      " [6.75588286]\n",
      " [6.57440567]\n",
      " [6.94550908]\n",
      " [6.50616872]\n",
      " [6.70066178]\n",
      " [6.66959608]\n",
      " [6.39625859]\n",
      " [6.74963307]\n",
      " [6.93015766]\n",
      " [6.672364  ]\n",
      " [6.55540013]\n",
      " [6.10020995]\n",
      " [7.17168653]\n",
      " [6.68803644]\n",
      " [6.97329342]\n",
      " [6.39874184]\n",
      " [6.71105063]\n",
      " [6.43413103]\n",
      " [6.2904067 ]\n",
      " [6.59958279]\n",
      " [6.24668777]\n",
      " [6.66168714]\n",
      " [6.9288373 ]\n",
      " [7.03595757]\n",
      " [6.77417529]\n",
      " [7.18589222]\n",
      " [7.49463427]\n",
      " [6.90050864]\n",
      " [6.16661215]\n",
      " [6.86775053]\n",
      " [6.14700317]\n",
      " [6.7676034 ]\n",
      " [6.82745659]\n",
      " [6.34529495]\n",
      " [7.00529802]\n",
      " [6.49479258]\n",
      " [6.98804581]\n",
      " [6.70505118]\n",
      " [6.8147043 ]\n",
      " [7.03467453]\n",
      " [7.02566898]\n",
      " [7.16839492]\n",
      " [6.62166488]\n",
      " [6.70344913]\n",
      " [6.89504921]\n",
      " [6.64998853]\n",
      " [6.71281135]\n",
      " [6.75853837]\n",
      " [6.67681503]\n",
      " [6.63727558]\n",
      " [6.41947162]\n",
      " [6.54252291]\n",
      " [6.72411752]\n",
      " [6.60977852]\n",
      " [6.61647129]\n",
      " [6.33306134]\n",
      " [7.23266542]\n",
      " [6.77520502]\n",
      " [6.66940355]\n",
      " [6.9758327 ]\n",
      " [7.06265473]\n",
      " [6.99597394]\n",
      " [6.9305644 ]\n",
      " [6.3822192 ]\n",
      " [6.49890316]\n",
      " [6.65544057]\n",
      " [6.83427286]\n",
      " [7.05346799]\n",
      " [6.80430377]\n",
      " [6.94901657]\n",
      " [6.66238272]\n",
      " [7.05671096]\n",
      " [6.48300087]\n",
      " [6.89013815]\n",
      " [6.61699951]\n",
      " [6.39334118]\n",
      " [6.81974947]\n",
      " [7.04652035]\n",
      " [7.08463204]\n",
      " [7.24856341]\n",
      " [6.74663067]\n",
      " [6.69671714]\n",
      " [6.90107632]\n",
      " [6.98025811]\n",
      " [6.77579439]\n",
      " [6.77507699]\n",
      " [6.71356547]\n",
      " [6.48061323]\n",
      " [6.72828948]\n",
      " [6.63483918]\n",
      " [7.13231421]\n",
      " [6.4634496 ]\n",
      " [6.99391794]\n",
      " [6.54631579]\n",
      " [6.66466808]\n",
      " [7.34862339]\n",
      " [7.07583177]\n",
      " [7.05566895]\n",
      " [6.62365413]\n",
      " [6.80528307]\n",
      " [6.05253446]\n",
      " [6.48704505]\n",
      " [6.70895839]\n",
      " [6.839535  ]\n",
      " [6.9703145 ]\n",
      " [6.71633756]\n",
      " [6.53792894]\n",
      " [6.34584105]\n",
      " [6.53244364]\n",
      " [6.46313119]\n",
      " [6.70723522]\n",
      " [6.5499804 ]\n",
      " [6.70761919]\n",
      " [6.92483222]\n",
      " [6.5798105 ]\n",
      " [5.83188975]\n",
      " [7.41667092]\n",
      " [6.677091  ]\n",
      " [6.31232083]\n",
      " [6.883582  ]\n",
      " [6.52871561]\n",
      " [7.38530886]\n",
      " [6.52300906]\n",
      " [6.77345765]\n",
      " [7.08942878]\n",
      " [7.04089558]\n",
      " [6.64852476]\n",
      " [6.48990357]\n",
      " [7.01442134]\n",
      " [6.5452826 ]\n",
      " [6.98941445]\n",
      " [6.75972009]\n",
      " [6.18424177]\n",
      " [6.68257344]\n",
      " [6.71461737]\n",
      " [6.71418786]\n",
      " [6.54288924]\n",
      " [6.94373047]\n",
      " [6.80659688]\n",
      " [6.52622879]\n",
      " [6.73130047]\n",
      " [6.39670879]\n",
      " [7.22402382]\n",
      " [7.06839383]\n",
      " [7.35895336]\n",
      " [6.52005553]\n",
      " [6.53329635]\n",
      " [6.47982168]\n",
      " [5.78719419]\n",
      " [7.08449686]\n",
      " [7.2423228 ]\n",
      " [6.62984574]\n",
      " [6.90009856]\n",
      " [6.49163318]\n",
      " [6.56377792]\n",
      " [6.83411837]\n",
      " [6.99069154]\n",
      " [7.01607132]\n",
      " [6.28583074]\n",
      " [6.57780397]\n",
      " [6.38987231]\n",
      " [6.71659863]\n",
      " [7.22862077]\n",
      " [6.78146756]\n",
      " [6.57487869]\n",
      " [6.87451398]\n",
      " [6.84830678]\n",
      " [6.14057827]\n",
      " [6.9426477 ]\n",
      " [6.19231522]\n",
      " [6.58565068]\n",
      " [7.08997524]\n",
      " [6.71704638]\n",
      " [6.88348269]\n",
      " [6.81903875]\n",
      " [6.45374274]\n",
      " [7.30611241]\n",
      " [6.73114181]\n",
      " [7.06118953]\n",
      " [6.46669388]\n",
      " [6.98642707]\n",
      " [6.49109173]\n",
      " [7.17957854]\n",
      " [6.55521739]\n",
      " [6.88295734]\n",
      " [6.90621161]\n",
      " [6.346017  ]\n",
      " [5.82761252]\n",
      " [6.45593929]\n",
      " [7.01292086]\n",
      " [6.68902373]\n",
      " [6.3840028 ]\n",
      " [6.41736996]\n",
      " [6.76244366]\n",
      " [6.82671535]\n",
      " [6.42244363]\n",
      " [7.50078678]\n",
      " [6.70545077]\n",
      " [6.227983  ]\n",
      " [6.79653168]\n",
      " [6.93110967]\n",
      " [5.95640957]\n",
      " [6.74434507]\n",
      " [6.60158801]\n",
      " [6.32848299]\n",
      " [6.38085675]\n",
      " [6.07796669]\n",
      " [6.09763706]\n",
      " [6.54894423]\n",
      " [6.88280022]\n",
      " [6.90151978]\n",
      " [6.8676666 ]\n",
      " [6.92369604]\n",
      " [5.95165658]\n",
      " [6.74923337]\n",
      " [6.70598328]\n",
      " [7.19490552]\n",
      " [6.69585335]\n",
      " [7.04837966]\n",
      " [6.7935251 ]\n",
      " [7.05779529]\n",
      " [7.15593195]\n",
      " [7.28741813]\n",
      " [6.78353798]\n",
      " [6.65772879]\n",
      " [6.99525774]\n",
      " [6.44887674]\n",
      " [6.73771918]\n",
      " [6.43340778]\n",
      " [7.23847699]\n",
      " [6.86567497]\n",
      " [6.35705519]\n",
      " [6.48026013]\n",
      " [6.8622613 ]\n",
      " [7.0278635 ]\n",
      " [6.84240782]\n",
      " [6.88777471]\n",
      " [6.77279985]\n",
      " [6.51110291]\n",
      " [6.5726887 ]\n",
      " [6.62679029]\n",
      " [6.50916171]\n",
      " [6.81949413]\n",
      " [6.70018816]\n",
      " [6.49099159]\n",
      " [6.42445171]\n",
      " [6.61334109]\n",
      " [6.45237041]\n",
      " [6.71424603]\n",
      " [6.47613025]\n",
      " [6.58647132]\n",
      " [6.83464944]\n",
      " [6.85316527]\n",
      " [6.25310779]\n",
      " [6.38441718]\n",
      " [6.66943181]\n",
      " [6.56247163]\n",
      " [6.32508647]\n",
      " [6.97668648]\n",
      " [6.99367392]\n",
      " [6.26794171]\n",
      " [6.82143927]\n",
      " [6.64905131]\n",
      " [6.55691612]\n",
      " [6.7210263 ]\n",
      " [6.93959367]\n",
      " [6.06445324]\n",
      " [6.90793145]\n",
      " [6.97417259]\n",
      " [6.79491127]\n",
      " [6.19356573]\n",
      " [6.77385366]\n",
      " [6.71799803]\n",
      " [7.53961492]\n",
      " [6.58570504]\n",
      " [6.90612805]\n",
      " [7.03702438]\n",
      " [6.90796077]\n",
      " [7.39328599]\n",
      " [6.79275668]\n",
      " [6.28329766]\n",
      " [6.97059202]\n",
      " [7.06296289]\n",
      " [6.65350389]\n",
      " [6.65923083]\n",
      " [6.52063727]\n",
      " [6.77436924]\n",
      " [6.49859917]\n",
      " [7.09997737]\n",
      " [7.15004611]\n",
      " [6.5161916 ]\n",
      " [6.78589845]\n",
      " [6.47774065]\n",
      " [6.52177513]\n",
      " [6.87785184]\n",
      " [6.68875885]\n",
      " [7.01069784]\n",
      " [6.50204372]\n",
      " [6.82605326]\n",
      " [6.58212626]\n",
      " [6.80544376]\n",
      " [6.74610484]\n",
      " [6.63393068]\n",
      " [6.66106963]\n",
      " [6.46911657]\n",
      " [6.61847544]\n",
      " [6.81928217]\n",
      " [6.80513358]\n",
      " [6.92146027]\n",
      " [6.8188988 ]\n",
      " [7.16441333]\n",
      " [6.5583787 ]\n",
      " [6.50985026]\n",
      " [6.79242063]\n",
      " [6.45730639]\n",
      " [6.23493695]\n",
      " [6.5393672 ]\n",
      " [6.67810631]\n",
      " [6.65402937]\n",
      " [6.93368757]\n",
      " [6.67076111]\n",
      " [6.45600379]\n",
      " [6.73193753]\n",
      " [6.45545185]\n",
      " [6.78433776]\n",
      " [6.28502345]\n",
      " [6.97168779]\n",
      " [6.8078258 ]\n",
      " [6.31379712]\n",
      " [6.79611719]\n",
      " [6.38382924]\n",
      " [6.23020399]\n",
      " [6.63253105]\n",
      " [6.8259269 ]\n",
      " [6.62675083]\n",
      " [6.90095544]\n",
      " [6.54130852]\n",
      " [6.74866641]\n",
      " [6.33166528]\n",
      " [6.93103123]\n",
      " [6.90733492]\n",
      " [6.72982728]\n",
      " [6.52987075]\n",
      " [6.31744921]\n",
      " [6.98655653]\n",
      " [6.38350868]\n",
      " [6.76294768]\n",
      " [7.18917143]\n",
      " [6.63384032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [6.39376575]]\n",
      "0.21285563751317177\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, actual):\n",
    "    margin_error = .25\n",
    "    num_correct = 0\n",
    "    for i in range(actual.size):\n",
    "        difference = abs(predictions[i] - actual[i])\n",
    "        if(difference <= margin_error):\n",
    "            num_correct += 1\n",
    "            \n",
    "    return num_correct/actual.size\n",
    "    \n",
    "print(calculate_accuracy(prediction, x_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
